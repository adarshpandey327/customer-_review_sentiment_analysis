{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Sentiment Analysis of Amazon Customer Reviews"]},{"cell_type":"markdown","metadata":{},"source":["This project uses the customer review data from Amazon.com Kindle store to perform a supervised binary (positive or negative) sentiment classification analysis. We use various data pre-processing techniques and three machine learning models, namely, Naive Bayes classification model, the Logistic regression model, and the linear support vector classification model. The result provides 87% prediction accuracy."]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{},"output_type":"display_data"}],"source":["import pyspark\n","spark.conf.set('spark.sql.shuffle.partitions', '8')"]},{"cell_type":"markdown","metadata":{},"source":["### Load dataset\n","The data comes from the website \"Amazon product data\" (http://jmcauley.ucsd.edu/data/amazon/) managed by Dr. Julian McAuley from UCSD. We choose the smaller subset of the customer review data from the Kindle store of Amazon.com. The data is in the JSON format, which contains 982,619 reviews and metadata spanning May 1996 - July 2014."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{},"output_type":"display_data"}],"source":["# load original .json data\n","kindle_json = spark.read.json('/FileStore/tables/Kindle_Store_5.json')"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">+----------+-------+-------+--------------------+----------+--------------+------------+------------------+--------------+\n","      asin|helpful|overall|          reviewText|reviewTime|    reviewerID|reviewerName|           summary|unixReviewTime|\n","+----------+-------+-------+--------------------+----------+--------------+------------+------------------+--------------+\n","B000F83SZQ| [0, 0]|    5.0|I enjoy vintage b...|05 5, 2014|A1F6404F1VG29J|  Avidreader|Nice vintage story|    1399248000|\n","B000F83SZQ| [2, 2]|    4.0|This book is a re...|01 6, 2014| AN0N05A9LIJEQ|    critters|      Different...|    1388966400|\n","B000F83SZQ| [2, 2]|    4.0|This was a fairly...|04 4, 2014| A795DMNCJILA6|         dot|             Oldie|    1396569600|\n","+----------+-------+-------+--------------------+----------+--------------+------------+------------------+--------------+\n","only showing top 3 rows\n","\n","</div>"]},"metadata":{},"output_type":"display_data"}],"source":["kindle_json.show(3)"]},{"cell_type":"markdown","metadata":{},"source":["### Generate Sentiment Label\n","\n","Reviews with overall rating of 1, 2, or 3 are labeled as negative (label=1), and reviews with overall rating of 4 or 5 are labeled as positive (label=0)."]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">+-----+------+\n","label| count|\n","+-----+------+\n","    1|153331|\n","    0|829250|\n","+-----+------+\n","\n","</div>"]},"metadata":{},"output_type":"display_data"}],"source":["kindle_json.createOrReplaceTempView('kindle_json_view')\n","\n","data_json = spark.sql('''\n","  SELECT CASE WHEN overall<4 THEN 1\n","          ELSE 0\n","          END as label,\n","        reviewText as text\n","  FROM kindle_json_view\n","  WHERE length(reviewText)>2''')\n","\n","data_json.groupBy('label').count().show()"]},{"cell_type":"markdown","metadata":{},"source":["### Generate the dataset for modeling\n","We only sample a small portion of the data for demonstration and try to balance the two classes."]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">+-----+-----+\n","label|count|\n","+-----+-----+\n","    1|38542|\n","    0|41226|\n","+-----+-----+\n","\n","</div>"]},"metadata":{},"output_type":"display_data"}],"source":["# Sampling data\n","pos = data_json.where('label=0').sample(False, 0.05, seed=1220)\n","neg = data_json.where('label=1').sample(False, 0.25, seed=1220)\n","data = pos.union(neg)\n","data.groupBy('label').count().show()"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">+-----+------------------+\n","label|avg(review_length)|\n","+-----+------------------+\n","    1| 622.5214311660008|\n","    0|  597.164071217193|\n","+-----+------------------+\n","\n","</div>"]},"metadata":{},"output_type":"display_data"}],"source":["# Negative reviews are on average longer than the positive reviews, but not significantly longer\n","from pyspark.sql.functions import length\n","data.withColumn('review_length', length('text')).groupBy('label').avg('review_length').show()"]},{"cell_type":"markdown","metadata":{},"source":["### Data Preprocessing\n","Data preprocessing process uses the following steps:\n","\n","* Use HTMLParser to un-escape the text\n","* Change \"can't\" to \"can not\", and change \"n't\" to \"not\" (This is useful for the negation handling process)\n","* Pad punctuations with blanks\n","* Lowercase every word\n","* Word tokenization\n","* Word lemmatization\n","* Perform **negation handling**\n","    * Use a state variable to store the negation state\n","    * Transform a word followed by a \"not\" or \"no\" into “not_” + word\n","    * Whenever the negation state variable is set, the words read are treated as “not_” + word\n","    * The state variable is reset when a punctuation mark is encountered or when there is double negation\n","* Use **bigram** and/or **trigram** models"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{},"output_type":"display_data"}],"source":["# Define preprocessing function\n","def clean(text):\n","    import html\n","    import string\n","    import nltk\n","    nltk.download('wordnet')\n","    \n","    line = html.unescape(text)\n","    line = line.replace(\"can't\", 'can not')\n","    line = line.replace(\"n't\", \" not\")\n","    # Pad punctuations with white spaces\n","    pad_punct = str.maketrans({key: \" {0} \".format(key) for key in string.punctuation}) \n","    line = line.translate(pad_punct)\n","    line = line.lower()\n","    line = line.split() \n","    lemmatizer = nltk.WordNetLemmatizer()\n","    line = [lemmatizer.lemmatize(t) for t in line] \n","    \n","    # Negation handling\n","    # Add \"not_\" prefix to words behind \"not\", or \"no\" until the end of the sentence\n","    tokens = []\n","    negated = False\n","    for t in line:\n","        if t in ['not', 'no']:\n","            negated = not negated\n","        elif t in string.punctuation or not t.isalpha():\n","            negated = False\n","        else:\n","            tokens.append('not_' + t if negated else t)\n","    \n","    invalidChars = str(string.punctuation.replace(\"_\", \"\"))  \n","    bi_tokens = list(nltk.bigrams(line))\n","    bi_tokens = list(map('_'.join, bi_tokens))\n","    bi_tokens = [i for i in bi_tokens if all(j not in invalidChars for j in i)]\n","    tri_tokens = list(nltk.trigrams(line))\n","    tri_tokens = list(map('_'.join, tri_tokens))\n","    tri_tokens = [i for i in tri_tokens if all(j not in invalidChars for j in i)]\n","    tokens = tokens + bi_tokens + tri_tokens      \n","    \n","    return tokens"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","[&apos;i&apos;, &apos;do&apos;, &apos;not_think&apos;, &apos;not_this&apos;, &apos;not_book&apos;, &apos;not_ha&apos;, &apos;not_any&apos;, &apos;not_decent&apos;, &apos;not_information&apos;, &apos;it&apos;, &apos;is&apos;, &apos;full&apos;, &apos;of&apos;, &apos;typo&apos;, &apos;and&apos;, &apos;factual&apos;, &apos;error&apos;, &apos;that&apos;, &apos;i&apos;, &apos;can&apos;, &apos;not_ignore&apos;, &apos;i_do&apos;, &apos;do_not&apos;, &apos;not_think&apos;, &apos;think_this&apos;, &apos;this_book&apos;, &apos;book_ha&apos;, &apos;ha_any&apos;, &apos;any_decent&apos;, &apos;decent_information&apos;, &apos;it_is&apos;, &apos;is_full&apos;, &apos;full_of&apos;, &apos;of_typo&apos;, &apos;typo_and&apos;, &apos;and_factual&apos;, &apos;factual_error&apos;, &apos;error_that&apos;, &apos;that_i&apos;, &apos;i_can&apos;, &apos;can_not&apos;, &apos;not_ignore&apos;, &apos;i_do_not&apos;, &apos;do_not_think&apos;, &apos;not_think_this&apos;, &apos;think_this_book&apos;, &apos;this_book_ha&apos;, &apos;book_ha_any&apos;, &apos;ha_any_decent&apos;, &apos;any_decent_information&apos;, &apos;it_is_full&apos;, &apos;is_full_of&apos;, &apos;full_of_typo&apos;, &apos;of_typo_and&apos;, &apos;typo_and_factual&apos;, &apos;and_factual_error&apos;, &apos;factual_error_that&apos;, &apos;error_that_i&apos;, &apos;that_i_can&apos;, &apos;i_can_not&apos;, &apos;can_not_ignore&apos;]\n","</div>"]},"metadata":{},"output_type":"display_data"}],"source":["# An example: how the function clean() pre-processes the input text\n","example = clean(\"I don't think this book has any decent information!!! It is full of typos and factual errors that I can't ignore.\")\n","print(example)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">+-----+--------------------+--------------------+\n","label|                text|              tokens|\n","+-----+--------------------+--------------------+\n","    0|I&apos;ve been an occa...|[i, ve, been, an,...|\n","    0|There is so much ...|[there, is, so, m...|\n","    0|Love this!  This ...|[love, this, this...|\n","+-----+--------------------+--------------------+\n","only showing top 3 rows\n","\n","</div>"]},"metadata":{},"output_type":"display_data"}],"source":["# Perform data preprocessing\n","from pyspark.sql.functions import udf, col, size\n","from pyspark.sql.types import ArrayType, StringType\n","clean_udf = udf(clean, ArrayType(StringType()))\n","data_tokens = data.withColumn('tokens', clean_udf(col('text')))\n","data_tokens.show(3)"]},{"cell_type":"markdown","metadata":{},"source":["### Split dataset to training (70%) and testing (30%) sets"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">+-----+-----+\n","label|count|\n","+-----+-----+\n","    1|26978|\n","    0|28737|\n","+-----+-----+\n","\n","</div>"]},"metadata":{},"output_type":"display_data"}],"source":["# Split data to 70% for training and 30% for testing\n","training, testing = data_tokens.randomSplit([0.7,0.3], seed=1220)\n","training.groupBy('label').count().show()"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">12</span><span class=\"ansired\">]: </span>DataFrame[label: int, text: string, tokens: array&lt;string&gt;]\n","</div>"]},"metadata":{},"output_type":"display_data"}],"source":["training.cache()"]},{"cell_type":"markdown","metadata":{},"source":["### Naive Bayes Model (with parameter tuning)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{},"output_type":"display_data"}],"source":["from pyspark.ml.feature import CountVectorizer, IDF\n","from pyspark.ml import Pipeline\n","\n","count_vec = CountVectorizer(inputCol='tokens', outputCol='c_vec', minDF=5.0)\n","idf = IDF(inputCol=\"c_vec\", outputCol=\"features\")"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n","label|                text|              tokens|               c_vec|            features|       rawPrediction|         probability|prediction|\n","+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n","    0|&quot;A Calm Whisper&quot;:...|[a, calm, whisper...|(237599,[0,1,2,3,...|(237599,[0,1,2,3,...|[-35891.606184157...|           [1.0,0.0]|       0.0|\n","    0|&quot;A Work in Progre...|[a, work, in, pro...|(237599,[0,1,2,3,...|(237599,[0,1,2,3,...|[-11666.318940416...|[1.0,2.1104994083...|       0.0|\n","    0|&quot;It&apos;s impossible ...|[it, s, impossibl...|(237599,[0,1,2,3,...|(237599,[0,1,2,3,...|[-11361.684192180...|[1.0,2.1713068771...|       0.0|\n","+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n","only showing top 3 rows\n","\n","</div>"]},"metadata":{},"output_type":"display_data"}],"source":["# Naive Bayes model\n","from pyspark.ml.classification import NaiveBayes\n","nb = NaiveBayes()\n","\n","pipeline_nb = Pipeline(stages=[count_vec, idf, nb])\n","\n","model_nb = pipeline_nb.fit(training)\n","test_nb = model_nb.transform(testing)\n","test_nb.show(3)"]},{"cell_type":"markdown","metadata":{},"source":["#### Naive Bayes model performance (using default parameters)\n","* Area under the ROC curve: 0.8551\n","* Accuracy: 0.8553"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">ROC of the NB model: 0.8550587747934197\n","</div>"]},"metadata":{},"output_type":"display_data"}],"source":["# Naive Bayes model ROC\n","from pyspark.ml.evaluation import BinaryClassificationEvaluator\n","roc_nb_eval = BinaryClassificationEvaluator(rawPredictionCol='prediction', labelCol='label')\n","roc_nb = roc_nb_eval.evaluate(test_nb)\n","print(\"ROC of the NB model: {}\".format(roc_nb))"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">Accuracy of the NB model: 0.8552779279092005\n","</div>"]},"metadata":{},"output_type":"display_data"}],"source":["# Naive Bayes model accuracy\n","from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n","acc_nb_eval = MulticlassClassificationEvaluator(metricName='accuracy')\n","acc_nb = acc_nb_eval.evaluate(test_nb)\n","print(\"Accuracy of the NB model: {}\".format(acc_nb))"]},{"cell_type":"markdown","metadata":{},"source":["#### Naive Bayes model performance after parameter tuning\n","* CountVectorizer.minDF = 7.0\n","* NaiveBayes.smooting = 1.0\n","* Accuracy: 0.8568 (increased from 0.8553)"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{},"output_type":"display_data"}],"source":["# NB parameter tuning and CV\n","from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n","\n","paramGrid_nb = (ParamGridBuilder()\n","                .addGrid(count_vec.minDF, [3.0, 5.0, 7.0, 10.0, 15.0])\n","                .addGrid(nb.smoothing, [0.1, 0.5, 1.0])\n","                .build())\n","cv_nb = CrossValidator(estimator=pipeline_nb, estimatorParamMaps=paramGrid_nb, evaluator=acc_nb_eval, numFolds=5)\n","cv_model_nb = cv_nb.fit(training) "]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">Accuracy of the NB CV model: 0.8568161975637134\n","</div>"]},"metadata":{},"output_type":"display_data"}],"source":["test_cv_nb = cv_model_nb.transform(testing)\n","acc_nb_cv = acc_nb_eval.evaluate(test_cv_nb)\n","print(\"Accuracy of the NB CV model: {}\".format(acc_nb_cv))"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">44</span><span class=\"ansired\">]: </span>\n","{Param(parent=&apos;CountVectorizer_44cda3f9b64132bd062e&apos;, name=&apos;inputCol&apos;, doc=&apos;input column name&apos;): &apos;tokens&apos;,\n"," Param(parent=&apos;CountVectorizer_44cda3f9b64132bd062e&apos;, name=&apos;binary&apos;, doc=&apos;If True, all non zero counts are set to 1.&apos;): False,\n"," Param(parent=&apos;CountVectorizer_44cda3f9b64132bd062e&apos;, name=&apos;vocabSize&apos;, doc=&apos;max size of the vocabulary&apos;): 262144,\n"," Param(parent=&apos;CountVectorizer_44cda3f9b64132bd062e&apos;, name=&apos;minDF&apos;, doc=&apos;Specifies the minimum number of different documents a term must appear in to be included in the vocabulary. If this is an integer &gt;= 1, this specifies the number of documents the term must appear in; if this is a double in [0,1), then this specifies the fraction of documents.&apos;): 7.0,\n"," Param(parent=&apos;CountVectorizer_44cda3f9b64132bd062e&apos;, name=&apos;outputCol&apos;, doc=&apos;output column name&apos;): &apos;c_vec&apos;,\n"," Param(parent=&apos;CountVectorizer_44cda3f9b64132bd062e&apos;, name=&apos;minTF&apos;, doc=&quot;Filter to ignore rare words in a document. For each document, terms with frequency/count less than the given threshold are ignored. If this is an integer &gt;= 1, then this specifies a count (of times the term must appear in the document); if this is a double in [0,1), then this specifies a fraction (out of the document&apos;s token count). Note that the parameter is only used in transform of CountVectorizerModel and does not affect fitting.&quot;): 1.0}\n","</div>"]},"metadata":{},"output_type":"display_data"}],"source":["cv_model_nb.bestModel.stages[0].extractParamMap()"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">45</span><span class=\"ansired\">]: </span>\n","{Param(parent=&apos;NaiveBayes_43c0b7665b211d226a53&apos;, name=&apos;predictionCol&apos;, doc=&apos;prediction column name&apos;): &apos;prediction&apos;,\n"," Param(parent=&apos;NaiveBayes_43c0b7665b211d226a53&apos;, name=&apos;rawPredictionCol&apos;, doc=&apos;raw prediction (a.k.a. confidence) column name&apos;): &apos;rawPrediction&apos;,\n"," Param(parent=&apos;NaiveBayes_43c0b7665b211d226a53&apos;, name=&apos;probabilityCol&apos;, doc=&apos;Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities&apos;): &apos;probability&apos;,\n"," Param(parent=&apos;NaiveBayes_43c0b7665b211d226a53&apos;, name=&apos;modelType&apos;, doc=&apos;The model type which is a string (case-sensitive). Supported options: multinomial (default) and bernoulli.&apos;): &apos;multinomial&apos;,\n"," Param(parent=&apos;NaiveBayes_43c0b7665b211d226a53&apos;, name=&apos;smoothing&apos;, doc=&apos;The smoothing parameter.&apos;): 1.0,\n"," Param(parent=&apos;NaiveBayes_43c0b7665b211d226a53&apos;, name=&apos;labelCol&apos;, doc=&apos;label column name&apos;): &apos;label&apos;,\n"," Param(parent=&apos;NaiveBayes_43c0b7665b211d226a53&apos;, name=&apos;featuresCol&apos;, doc=&apos;features column name&apos;): &apos;features&apos;}\n","</div>"]},"metadata":{},"output_type":"display_data"}],"source":["cv_model_nb.bestModel.stages[2].extractParamMap()"]},{"cell_type":"markdown","metadata":{},"source":["### Logistic Regressions\n","Model performance (using default parameters)\n","* Area under the ROC curve: 0.8601\n","* Accuracy: 0.8610"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{},"output_type":"display_data"}],"source":["# Logistic Regression model\n","from pyspark.ml.classification import LogisticRegression\n","lgr = LogisticRegression(maxIter=5)\n","pipeline_lgr = Pipeline(stages=[count_vec, idf, lgr])\n","\n","model_lgr = pipeline_lgr.fit(training)\n","test_lgr = model_lgr.transform(testing)"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">ROC of the model: 0.8601176818374295\n","</div>"]},"metadata":{},"output_type":"display_data"}],"source":["# Logistic Regression model ROC\n","from pyspark.ml.evaluation import BinaryClassificationEvaluator\n","roc_lgr_eval = BinaryClassificationEvaluator(rawPredictionCol='prediction', labelCol='label')\n","roc_lgr = roc_lgr_eval.evaluate(test_lgr)\n","print(\"ROC of the model: {}\".format(roc_lgr))"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">Accuracy of the model: 0.8609736831164512\n","</div>"]},"metadata":{},"output_type":"display_data"}],"source":["# Logistic Regression model accuracy\n","#from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n","acc_lgr_eval = MulticlassClassificationEvaluator(metricName='accuracy')\n","acc_lgr = acc_lgr_eval.evaluate(test_lgr)\n","print(\"Accuracy of the model: {}\".format(acc_lgr))"]},{"cell_type":"markdown","metadata":{},"source":["### Linear SVC Model\n","Model performance (using default parameters)\n","* Area under the ROC curve: 0.8649\n","* Accuracy: 0.8656"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{},"output_type":"display_data"}],"source":["# Linear SVC model\n","from pyspark.ml.classification import LinearSVC\n","lsvc = LinearSVC(maxIter=5)\n","pipeline_lsvc = Pipeline(stages=[count_vec, idf, lsvc])\n","\n","model_lsvc = pipeline_lsvc.fit(training)\n","test_lsvc = model_lsvc.transform(testing)"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">ROC of the model: 0.8648930463858352\n","</div>"]},"metadata":{},"output_type":"display_data"}],"source":["# Linear SVC model ROC\n","from pyspark.ml.evaluation import BinaryClassificationEvaluator\n","roc_lsvc_eval = BinaryClassificationEvaluator(rawPredictionCol='prediction', labelCol='label')\n","roc_lsvc = roc_lsvc_eval.evaluate(test_lsvc)\n","print(\"ROC of the model: {}\".format(roc_lsvc))"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">Accuracy of the model: 0.8656300669355174\n","</div>"]},"metadata":{},"output_type":"display_data"}],"source":["# Linear SVC model accuracy\n","#from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n","acc_lsvc_eval = MulticlassClassificationEvaluator(metricName='accuracy')\n","acc_lsvc = acc_lsvc_eval.evaluate(test_lsvc)\n","print(\"Accuracy of the model: {}\".format(acc_lsvc))"]},{"cell_type":"markdown","metadata":{},"source":["### Predict on new reviews:\n","To demonstrate the model prediction on new review texts, I randomly choose five reviews from the Kindle book *The Brave Ones: A Memoir of Hope, Pride and Military Service, by Michael J. MacLeod*. \n","\n","The suffixes \"_1\", \"_2\", ..., \"_5\" indicate the real overall review stars 1, 2, ..., 5.\n","\n","The model correctly predicts the first three reviews as \"negative\" (label=1), and the last two as \"positive\" (label=0)."]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{},"output_type":"display_data"}],"source":["review_1 = [\"WOW!!! No words describe how bland this book is. It took me a lot to even pick up to read. I would definitely not recommend this book.\"]"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{},"output_type":"display_data"}],"source":["review_2 = [\"A first person account of the war in Afghanistan. It skipps around a lot and is like a never-ending news article. On the positive side, you do get a feel for what desert fighting is like from a soldiers point of view.\"]"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{},"output_type":"display_data"}],"source":["review_3 = [\"I liked the premise and most of the book. At the end parts I lost a little interest because I lost the thread of who was who. War is hell. MacLeod did his service unlike most of us.\"]"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{},"output_type":"display_data"}],"source":["review_4 = [\"Very informative first person account of the the daily life of a US Paratrooper. From training to deployment in combat situations in Afghanistan. Well worth the read and makes you really understand and appreciate their sacrifices\"]"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{},"output_type":"display_data"}],"source":["review_5 = [\"This is perhaps the best wrote book I have ever read. Articulate and thought provoking. Not just a riveting account of actual combat, but Michael was able to do what few before him have...captured the essence of what one feels as the battle unfolds. Perhaps most of all, I am grateful to call this author 'Fellow Warrior' Airborne all the way!!!\"]"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{},"output_type":"display_data"}],"source":["from pyspark.sql.types import *\n","schema = StructType([StructField(\"text\", StringType(), True)])\n","\n","text = [review_1, review_2, review_3, review_4, review_5]\n","review_new = spark.createDataFrame(text, schema=schema)"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">+--------------------+--------------------+\n","                text|              tokens|\n","+--------------------+--------------------+\n","WOW!!! No words d...|[wow, not_word, n...|\n","A first person ac...|[a, first, person...|\n","I liked the premi...|[i, liked, the, p...|\n","Very informative ...|[very, informativ...|\n","This is perhaps t...|[this, is, perhap...|\n","+--------------------+--------------------+\n","\n","</div>"]},"metadata":{},"output_type":"display_data"}],"source":["# Data preprocessing\n","review_new_tokens = review_new.withColumn('tokens', clean_udf(col('text')))\n","review_new_tokens.show()"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">+--------------------+----------+\n","                text|prediction|\n","+--------------------+----------+\n","WOW!!! No words d...|       1.0|\n","A first person ac...|       1.0|\n","I liked the premi...|       1.0|\n","Very informative ...|       0.0|\n","This is perhaps t...|       0.0|\n","+--------------------+----------+\n","\n","</div>"]},"metadata":{},"output_type":"display_data"}],"source":["# Prediction using tuned Naive Bayes model\n","result = cv_model_nb.transform(review_new_tokens)\n","result.select('text', 'prediction').show()"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[],"source":[]}],"metadata":{"language_info":{"name":"python"},"name":"nlp_json","notebookId":190787089418947},"nbformat":4,"nbformat_minor":0}
